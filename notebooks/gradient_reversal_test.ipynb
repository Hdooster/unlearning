{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\n",
    "# ms-python.python added\n",
    "import os\n",
    "try:\n",
    "\tos.chdir(os.path.join(os.getcwd(), '../src'))\n",
    "\tprint(os.getcwd())\n",
    "except:\n",
    "\tpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from network import basenet, get_multitask_network, get_numberonly_network, \\\n",
    "    get_multitask_network_gradflip\n",
    "from utils import colors as all_colors_rgb, color_MNIST, compile_model, evaluate_results\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "keras.backend.tensorflow_backend.set_session(sess)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR = 1e-4\n",
    "EPOCHS = 30\n",
    "PATIENCE = EPOCHS  # no early stopping here\n",
    "BIAS = 0.5\n",
    "GRADFLIP_LAMBDA = .2\n",
    "DATA_FOLDER = '../data'\n",
    "LOSS = 'categorical_crossentropy'\n",
    "N = 200\n",
    "\n",
    "# choose some colors\n",
    "colors = {0: 'dark red',\n",
    "          1: 'navy',\n",
    "          2: 'gold',\n",
    "          3: 'aqua',\n",
    "          4: 'indigo',\n",
    "          5: 'deep pink',\n",
    "          6: 'chocolate',\n",
    "          7: 'honeydew',\n",
    "          8: 'dark violet',\n",
    "          9: 'beige'\n",
    "          }\n",
    "colors_inv = {v: k for k, v in colors.items()}\n",
    "colors_rgb = {k: all_colors_rgb[v] for k, v in colors.items()}\n",
    "classes = list(colors.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color-biased MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)\n",
    "\n",
    "x_train_color, y_train_color = color_MNIST(zip(x_train, y_train), colors, colors_rgb, bias=BIAS)\n",
    "x_test_color, y_test_color = color_MNIST(zip(x_test, y_test), colors, colors_rgb, bias=0)\n",
    "x_test_color_ref, y_test_color_ref = color_MNIST(zip(x_test, y_test), colors, colors_rgb, bias=BIAS)\n",
    "\n",
    "y_train_color_onehot = to_categorical([colors_inv[c] for c in y_train_color])\n",
    "y_train_multi = {'color': y_train_color_onehot, 'number': y_train_onehot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write away some results to jpg for inspection\n",
    "shutil.rmtree(DATA_FOLDER)\n",
    "for f in ['train', 'test']:\n",
    "    [os.makedirs(os.path.join(DATA_FOLDER, f, str(label)), exist_ok=True)\n",
    "     for label in classes]\n",
    "    [os.makedirs(os.path.join(DATA_FOLDER, f, str(label)), exist_ok=True)\n",
    "     for label in classes]\n",
    "\n",
    "for i in range(N):\n",
    "    path = os.path.join(DATA_FOLDER, 'train', str(y_train[i]), str(i)+'.jpg')\n",
    "    ret = cv2.imwrite(path, x_train_color[i])\n",
    "    assert ret\n",
    "\n",
    "for i in range(N):\n",
    "    path = os.path.join(DATA_FOLDER, 'test', str(y_test[i]), str(i)+'.jpg')\n",
    "    ret = cv2.imwrite(path, x_test_color[i])\n",
    "    assert ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## train: number only, confirm bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = basenet(input_shape=(28, 28, 3), n_conv=3,\n",
    "                    init_filter_size=20, dropout_rate=0)\n",
    "model_number = get_numberonly_network(backbone=simplenet)\n",
    "model_number, callbacks = compile_model(model_number, 'mnist_number',\n",
    "                                        loss=LOSS,  initial_lr=INITIAL_LR, patience=PATIENCE)\n",
    "model_number.fit(x_train_color, y_train_onehot, validation_split=.2,\n",
    "                 epochs=EPOCHS, callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without bias\n",
    "y_pred = np.argmax(model_number.predict(x_test_color), axis=1)\n",
    "evaluate_results(y_test, y_pred, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our model has become worse in predicting the number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train: Gradient Reversal\n",
    " In https://github.com/feidfoe/learning-not-to-learn/blob/master/trainer.py we see the authors train with a minimax game and gradient reversal.\n",
    "\n",
    " In essence, this means the head for color still tries to extract color info from the shared embedding, but during backprop we flip the gradient between the start of the color head and the embedding layer, meaning the shared weights move away from allowing encoding color information.\n",
    "\n",
    " Let's try the gradient reversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = basenet(input_shape=(28, 28, 3), n_conv=3,\n",
    "                    init_filter_size=20, dropout_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gradflip = get_multitask_network_gradflip(simplenet, gradflip_lambda=GRADFLIP_LAMBDA)\n",
    "\n",
    "LOSS = 'categorical_crossentropy'\n",
    "loss_weights = {\n",
    "    \"number\": 1,\n",
    "    \"color\": 1\n",
    "}\n",
    "\n",
    "model_gradflip, callbacks = compile_model(model_gradflip, 'mnist_number_color', LOSS,\n",
    "                                          loss_weights=loss_weights, initial_lr=INITIAL_LR, patience=PATIENCE)\n",
    "model_gradflip.fit(x_train_color, y_train_multi, validation_split=.2,\n",
    "                   epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, lets predict on the inconsistently colored testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no consistent coloring in the testset, contrary to the trainingset\n",
    "# todo: how are we guaranteed of the order of outputs?\n",
    "y_pred_number, y_pred_color = model_gradflip.predict(x_test_color)\n",
    "y_pred_number = np.argmax(y_pred_number, axis=1)\n",
    "evaluate_results(y_test, y_pred_number, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Following settings:  \n",
    "INITIAL_LR = 1e-4  \n",
    "EPOCHS = 50  \n",
    "PATIENCE = EPOCHS  \n",
    "BIAS = 0.9  \n",
    "GRADFLIP_LAMBDA = .1  \n",
    "\n",
    "Our testset accuracy in predicting the number went up from 84.33 to 90.4 percent, which is pretty decent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Notes:\n",
    " * Bias cannot be set to 1. If it is 1, color and number information align exactly and the two heads fight for keeping/destroying that information, without being able to nuance. Noisy bias ensures that there is indeed color info in the embedding, **separable** from number info.\n",
    " * Use a more powerful color branch (more dense layers) than the number branch so that simple hiding of color info in the embedding isn't enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "ludwig",
   "language": "python",
   "name": "ludwig"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
