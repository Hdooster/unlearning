{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:  \n",
    "http://jmlr.org/papers/volume17/15-239/15-239.pdf  \n",
    "https://github.com/michetonu/DA-RNN_manoeuver_anticipation  \n",
    "https://github.com/michetonu/gradient_reversal_keras_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "import shutil\n",
    "\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications import MobileNetV2\n",
    "from keras import backend as K, Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Conv2D, pooling, Dropout, Dense, Input, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import SGD\n",
    "from keras.engine import Layer\n",
    "\n",
    "# import stuff in this project\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "if parentdir not in sys.path:\n",
    "    sys.path.insert(0,parentdir)\n",
    "from colored_MNIST import colors as all_colors_rgb\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colored_MNIST_output_folder = './colored_MNIST'\n",
    "colors = {0: 'dark red', \n",
    "          1: 'navy',\n",
    "          2: 'gold',\n",
    "          3: 'aqua',\n",
    "          4: 'indigo',\n",
    "          5: 'deep pink',\n",
    "          6: 'chocolate',\n",
    "          7: 'honeydew',\n",
    "          8: 'dark violet',\n",
    "          9: 'beige'\n",
    "         }\n",
    "colors_inv = {v:k for k,v in colors.items()}\n",
    "colors_rgb = {k: all_colors_rgb[v] for k, v in colors.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR = 1e-4\n",
    "EPOCHS = 4\n",
    "BIAS = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation\n",
    "We will color the MNIST dataset, and will add the color label to the labels as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_MNIST(gray_dataset, bias=0, color_noise=False):\n",
    "    ''' this function takes single-channel np.uint8 0-255 images and their labels, \n",
    "    and returns RGB np.uint8 0-255 images and their colors, optionally correlating colors to the labels.\n",
    "    The goal of this function is to introduce bias into the MNIST dataset.'''\n",
    "    \n",
    "    x = []\n",
    "    y_color = []\n",
    "    \n",
    "    for img, label in gray_dataset:\n",
    "        if np.random.rand() < bias:\n",
    "            i = label\n",
    "        else:\n",
    "            i = random.choice(range(10))\n",
    "            \n",
    "        icolor = colors[i]\n",
    "        icolor_rgb = colors_rgb[i]\n",
    "        cimg = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        cimg = cimg.astype(np.float32)/255.\n",
    "\n",
    "        # convert color\n",
    "        cimg[..., 0] = cimg[..., 0] * icolor_rgb[0]\n",
    "        cimg[..., 1] = cimg[..., 1] * icolor_rgb[1]\n",
    "        cimg[..., 2] = cimg[..., 2] * icolor_rgb[2]\n",
    "        cimg = cimg.astype(np.uint8)\n",
    "        x.append(cimg)\n",
    "        y_color.append(icolor)\n",
    "        \n",
    "    return np.array(x), np.array(y_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_color, y_train_color = color_MNIST(zip(x_train, y_train), bias=BIAS)\n",
    "x_test_color, y_test_color = color_MNIST(zip(x_test, y_test), bias=0)  # here, our real-world testset won't have the bias\n",
    "x_test_color_ref, y_test_color_ref = color_MNIST(zip(x_test, y_test), bias=BIAS)  # this to check what what if it does.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_color_onehot = to_categorical([colors_inv[c] for c in y_train_color])\n",
    "y_train_multi = {\n",
    "    'color': y_train_color_onehot,\n",
    "    'number': y_train_onehot\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(range(1000))\n",
    "\n",
    "plt.imshow(x_test_color[i])\n",
    "print(y_test[i], y_test_color[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_test_color_ref[i])\n",
    "print(y_test[i], y_test_color_ref[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write away some results to jpg\n",
    "shutil.rmtree(colored_MNIST_output_folder)\n",
    "for f in ['train', 'test']:\n",
    "    [os.makedirs(os.path.join(colored_MNIST_output_folder, f, str(label)), exist_ok=True) for label in range(10)]    \n",
    "    [os.makedirs(os.path.join(colored_MNIST_output_folder, f, str(label)), exist_ok=True) for label in range(10)]\n",
    "\n",
    "N = 200\n",
    "\n",
    "for i in range(N):\n",
    "    path=os.path.join(colored_MNIST_output_folder, 'train', str(y_train[i]), str(i)+'.jpg')\n",
    "    ret = cv2.imwrite(path, x_train_color[i])\n",
    "    assert ret\n",
    "    \n",
    "for i in range(N):\n",
    "    path=os.path.join(colored_MNIST_output_folder, 'test', str(y_test[i]), str(i)+'.jpg')\n",
    "    ret = cv2.imwrite(path, x_test_color[i])\n",
    "    assert ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_base_classifier_2(input_shape=(None,None,3), n_conv=3,\n",
    "                             init_filter_size=10, dropout_rate=0.10,\n",
    "                             conv1x1_filters=None, include_top=False, \n",
    "                             hidden_units=None, n_classes=None):\n",
    "    \n",
    "    x = inp = Input(shape=input_shape)\n",
    "    \n",
    "    for i in range(0, n_conv, 1):\n",
    "        x = Conv2D(filters=init_filter_size * (2**i), kernel_size=3, activation='relu')(x)\n",
    "        if dropout_rate:\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "        if i < n_conv - 1:\n",
    "            x = pooling.MaxPool2D()(x)\n",
    "    if conv1x1_filters:\n",
    "        x = Conv2D(filters=conv1x1_filters, kernel_size=1, activation='relu')(x)\n",
    "    \n",
    "    x = pooling.GlobalAvgPool2D()(x)\n",
    "\n",
    "    if include_top:\n",
    "        if isinstance(hidden_units, (list, tuple)):\n",
    "            for units in hidden_units:\n",
    "                x = Dense(units, activation='relu')(x)\n",
    "        else:\n",
    "            x = Dense(hidden_units, activation='relu')(x)\n",
    "        x = Dense(n_classes, activation=\"sigmoid\")(x)\n",
    "    model = Model(inp, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_multihead_branch(inputs, num_classes, final_act, l2_norm=False,\n",
    "                         branch_name=None, dense=True):\n",
    "    x = inputs\n",
    "    if dense:\n",
    "        x = Dense(20, activation='relu')(x)\n",
    "    x = Dense(num_classes, activation=final_act, name=branch_name or final_act)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_multitask_network(backbone=MobileNetV2, num_classes=10):\n",
    "    outputs = backbone.output\n",
    "    number = get_multihead_branch(outputs, num_classes, final_act='softmax', branch_name='number', l2_norm=False)\n",
    "    color = get_multihead_branch(outputs, num_classes, final_act='softmax', branch_name='color', dense=False)\n",
    "    model = Model(backbone.input, [number, color], name='number_color_model')\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_numberonly_network(backbone=MobileNetV2, num_classes=10):\n",
    "    outputs = backbone.output\n",
    "\n",
    "    number = get_multihead_branch(outputs, num_classes, final_act='softmax', branch_name='number', l2_norm=False)    \n",
    "    model = Model(backbone.input, [number], name='number_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_save_callbacks(name, log=True, save=True):\n",
    "    # set up callbacks\n",
    "    callbacks = []\n",
    "    save_dir_root = \"../model_training/\"\n",
    "\n",
    "    if save:\n",
    "        fp_modelcheckpoint = os.path.join(save_dir_root, \"modelcheckpoints\")\n",
    "        os.makedirs(fp_modelcheckpoint, exist_ok=True)\n",
    "        h5_filename = os.path.join(fp_modelcheckpoint, name + \".hdf5\")\n",
    "        callbacks += [ModelCheckpoint(h5_filename, save_best_only=True)]\n",
    "\n",
    "    if log:\n",
    "        dir_tensorboard = os.path.join(save_dir_root, \"tensorboard\", name)\n",
    "        os.makedirs(dir_tensorboard, exist_ok=True)\n",
    "        callbacks += [TensorBoard(dir_tensorboard)]\n",
    "\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "def compile_model(model, name, loss, loss_weights=None, initial_lr=1e-5):\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    callbacks = log_save_callbacks(name=name + '_' + timestamp, log=True, save=False)\n",
    "    callbacks += [EarlyStopping(patience=20, verbose=1, restore_best_weights=True)]\n",
    "    callbacks += [ReduceLROnPlateau(verbose=1, factor=0.2, patience=10)]\n",
    "\n",
    "#     optimizer = Adam(lr=initial_lr)\n",
    "    optimizer = SGD(initial_lr)\n",
    "    model.compile(optimizer, loss=loss, metrics=['acc'], loss_weights=loss_weights)\n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(y_true, y_pred, all_labels):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=all_labels)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=all_labels, yticklabels=all_labels,\n",
    "           ylim=(len(all_labels)-0.5, -0.5),\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute accuracy\n",
    "    acc = np.mean(y_true==y_pred)\n",
    "    print(f'Accuracy: {acc*100.:.2f}%')\n",
    "    print('Per class statistics:')\n",
    "    # Compute statistics per defect type\n",
    "    for i, label in enumerate(all_labels):\n",
    "        totp = np.sum(cm[:, i])\n",
    "        realp = np.sum(cm[i])\n",
    "        tp = cm[i][i]\n",
    "        fp = totp - tp\n",
    "        fn = realp - tp\n",
    "        prec = tp / (tp+fp)\n",
    "        rec = tp / realp\n",
    "        print(f'  - {label}: precision {prec*100.:.2f}%, recall {rec*100.:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train: number only, confirm bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = simple_base_classifier_2(input_shape=(28, 28, 3), n_conv=3, init_filter_size=10, dropout_rate=0)\n",
    "model = get_numberonly_network(backbone=simplenet)\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "loss_weights = {\"number\": 1}\n",
    "model, callbacks = compile_model(model, 'mnist_simplenet_number',\n",
    "                                 loss, loss_weights=loss_weights,  # or loss_weights=None\n",
    "                                 initial_lr=INITIAL_LR)\n",
    "\n",
    "model.fit(x_train_color, y_train_onehot, epochs=EPOCHS, callbacks=callbacks)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consistent colored as in dataset\n",
    "y_pred_ref = np.argmax(model.predict(x_test_color_ref), axis=1)\n",
    "# y_pred_ref = [colors[i] for i in y_pred_ref]\n",
    "evaluate_results(y_test, y_pred_ref, range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no consistent coloring in the testset, contrary to the trainingset\n",
    "y_pred = np.argmax(model.predict(x_test_color), axis=1)\n",
    "evaluate_results(y_test, y_pred, range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has become worse in predicting the number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train: both number and color, flip color loss negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = simple_base_classifier_2(input_shape=(28, 28, 3), n_conv=3, init_filter_size=10, dropout_rate=0)\n",
    "model_debiased = get_multitask_network(backbone=simplenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'categorical_crossentropy'\n",
    "negative_color_weights = {\n",
    "    \"number\": 1, \n",
    "    \"color\": -.1\n",
    "}\n",
    "\n",
    "model_debiased, callbacks = compile_model(model_debiased, 'mnist_simplenet_number_color', loss, \n",
    "                                          loss_weights=negative_color_weights, initial_lr=INITIAL_LR)\n",
    "model_debiased.fit(x_train_color, y_train_multi, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consistent colored testset\n",
    "y_pred_number_ref, y_pred_color_ref = model_debiased.predict(x_test_color_ref)\n",
    "y_pred_number_ref = np.argmax(y_pred_number_ref, axis=1)\n",
    "evaluate_results(y_test, y_pred_number_ref, range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict numbers pretty well on the reference dataset, except for 4 and 8, which are confused (indigo and dark violet are close to each other, and so our bias screws up results on the testset even if we color it consistently.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets predict on the inconsistently colored testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no consistent coloring in the testset, contrary to the trainingset\n",
    "y_pred_number, y_pred_color = model_debiased.predict(x_test_color)  # todo: how are we guaranteed of the order of outputs?\n",
    "y_pred_number = np.argmax(y_pred_number, axis=1)\n",
    "evaluate_results(y_test, y_pred_number, range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like a negative loss weight for 'color' is not enough... We  are probably just scrambling the crap out of our last layer\n",
    "\n",
    "### train: Gradient Reversal\n",
    "In https://github.com/feidfoe/learning-not-to-learn/blob/master/trainer.py we see the authors train with a minimax game and gradient reversal.  \n",
    "  \n",
    "In essence, this means the head for color still tries to extract color info from the shared embedding, but during backprop we flip the gradient between the start of the color head and the embedding layer, meaning the shared weights move away from allowing encoding color information.  \n",
    "  \n",
    "Let's try the gradient reversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_gradient(X, hp_lambda):\n",
    "    '''Flips the sign of the incoming gradient during training.'''\n",
    "    try:\n",
    "        reverse_gradient.num_calls += 1\n",
    "    except AttributeError:\n",
    "        reverse_gradient.num_calls = 1\n",
    "\n",
    "    grad_name = \"GradientReversal%d\" % reverse_gradient.num_calls\n",
    "\n",
    "    @tf.RegisterGradient(grad_name)\n",
    "    def _flip_gradients(op, grad):\n",
    "        return [tf.negative(grad) * hp_lambda]\n",
    "\n",
    "    g = K.get_session().graph\n",
    "    with g.gradient_override_map({'Identity': grad_name}):\n",
    "        y = tf.identity(X)\n",
    "\n",
    "    return y\n",
    "\n",
    "class GradientReversal(Layer):\n",
    "    '''Flip the sign of gradient during training.'''\n",
    "    def __init__(self, hp_lambda, **kwargs):\n",
    "        super(GradientReversal, self).__init__(**kwargs)\n",
    "        self.supports_masking = False\n",
    "        self.hp_lambda = hp_lambda\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.trainable_weights = []\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return reverse_gradient(x, self.hp_lambda)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'hp_lambda': self.hp_lambda}\n",
    "        base_config = super(GradientReversal, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multihead_branch_gradrev(inputs, num_classes, final_act, l2_norm=False,\n",
    "                         branch_name=None, dense=True, reverse_grad=False, hp_lambda=.1):\n",
    "    x = inputs\n",
    "    if reverse_grad:\n",
    "        # a gradient reversal layer, useful if we want the base part of a network to be penalized for \n",
    "        # encoding information on which this branch can predict its output.\n",
    "        flip_layer = GradientReversal(hp_lambda)\n",
    "        x = flip_layer(x)\n",
    "        print(branch_name, ' reverse grad from here')\n",
    "        \n",
    "    if dense:\n",
    "        x = Dense(20, activation='relu')(x)\n",
    "    x = Dense(num_classes, activation=final_act, name=branch_name or final_act)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_multitask_network_gradflip(backbone=MobileNetV2, num_classes=10):\n",
    "    outputs = backbone.output\n",
    "    number = get_multihead_branch_gradrev(outputs, num_classes, final_act='softmax', \n",
    "                                          branch_name='number', reverse_grad=False, dense=False)\n",
    "    \n",
    "    color = get_multihead_branch_gradrev(outputs, num_classes, final_act='softmax', \n",
    "                                         branch_name='color', reverse_grad=True, hp_lambda=.1)\n",
    "    \n",
    "    model = Model(backbone.input, [number, color], name='number_color_gradflip')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = simple_base_classifier_2(input_shape=(28, 28, 3), n_conv=3, init_filter_size=10, dropout_rate=0)\n",
    "model_gradflip = get_multitask_network_gradflip(simplenet)\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "loss_weights = {\n",
    "    \"number\": 1, \n",
    "    \"color\": 1\n",
    "}\n",
    "\n",
    "model_gradflip, callbacks = compile_model(model_gradflip, 'mnist_simplenet_number_color', loss, \n",
    "                                          loss_weights=loss_weights, initial_lr=INITIAL_LR)\n",
    "model_gradflip.fit(x_train_color, y_train_multi, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consistent colored testset\n",
    "y_pred_number_ref, y_pred_color_ref = model_gradflip.predict(x_test_color_ref)\n",
    "y_pred_number_ref = np.argmax(y_pred_number_ref, axis=1)\n",
    "evaluate_results(y_test, y_pred_number_ref, range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With hp_lambda=1 we see that 4 is always predicted as 8, so there is prediction based on colors going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets predict on the inconsistently colored testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no consistent coloring in the testset, contrary to the trainingset\n",
    "y_pred_number, y_pred_color = model_gradflip.predict(x_test_color)  # todo: how are we guaranteed of the order of outputs?\n",
    "y_pred_number = np.argmax(y_pred_number, axis=1)\n",
    "evaluate_results(y_test, y_pred_number, range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results, we are getting somewhere!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo: \n",
    "* BIAS MUST CONTAIN NOISE. THE NUMBER ITSELF OFCOURSE ENCODES COLOR IF THERE IS NO NOISE, so the gradflip layer doesn't matter, we by definition have the same info as the number branch which is trained to predict number. Noisy bias ensures that there is indeed color info in the embedding, **separate** from number info.\n",
    "* Confirm simplenet can train on structure alone by training it on original MNIST  \n",
    "* Use a more powerful color branch (more dense layers) than the number branch so that simple hiding of color info in the embedding isn't enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rvai",
   "language": "python",
   "name": "rvai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
